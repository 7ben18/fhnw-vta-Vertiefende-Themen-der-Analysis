{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vta - Mini Challenge - Gradient Descent \n",
    "\n",
    "Das Ziel dieser Aufgabe besteht darin, dass Sie ein grundlegendes Verständnis für numerische\n",
    "Näherungsverfahren in höheren Dimensionen erlangen, insbesondere für den Gradient Descent\n",
    "und dessen praktische Anwendung. Hierfür sollen Sie ein Jupyter Notebook erstellen und das\n",
    "MNIST Dataset laden und erkunden. Anschließend sollen Sie ein neuronales Netzwerk erstellen\n",
    "und trainieren, um die Bilder korrekt zu klassifizieren. Es dürfen nur die angegebenen Python\n",
    "packages verwendet werden.  \n",
    "Ziel dieser Aufgabe ist nicht nur, Ihre mathematischen Kenntnisse unter Beweis zu stellen, sondern\n",
    "auch die entsprechende Kommunikation und Präsentation Ihrer Ergebnisse. Ihre Abgaben sollen\n",
    "also nicht nur mathematisch korrekt, sondern auch leicht verständlich und reproduzierbar\n",
    "sein. Genauere Angaben zu den Erwartungen an die Abgabe finden Sie in den Auswertungskriterien.\n",
    "Dokumentieren Sie ihren Arbeitsfortschritt und Erkenntnisgewinn in Form eines Lerntagebuchs,\n",
    "um Lernfortschritte, Schwierigkeiten und Erkenntnisse festzuhalten.\n",
    "Die folgenden Aufgabenstellungen präzisieren die einzelnen Bearbeitungsschritte und geben die\n",
    "Struktur des Notebooks vor.\n",
    "\n",
    "Diese Challenge wurde im FS23 erarbeitet von:  \n",
    "Patrik Schürmann   \n",
    "Tobias Buess  \n",
    "Si Ben Tran   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Aufgabe 1 \n",
    "Laden Sie das MNIST-Dataset mithilfe des torchvision-Pakets (Verwenden Sie das torchvision\n",
    "Paket für diese Aufgabe) und verwenden Sie matplotlib, um sich einen Überblick über die Daten\n",
    "zu verschaffen. Beschreiben Sie das grundlegenden Eigenschaften des Datensets, z.B. wie viele\n",
    "und welche Daten es enthält.\n",
    "\n",
    "**Dataset**\n",
    "1) Sind Trainings und Testdaten des MNIST-Dataset korrekt mithilfe des torchvision-Pakets\n",
    "geladen worden?\n",
    "2) Die grundlegenden Eigenschaften des MNIST-Datasets werden richtig beschrieben.\n",
    "3) Die Visualisierungen der Daten sind gut verständlich und representativ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Laden \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Trainings und Testdaten des MNIST-Dataset mithilfe torchvision laden\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Eigenschaften des MNIST-Datasets beschreiben\n",
    "\n",
    "# Ausgabe der Trainings- und Testdaten shape\n",
    "print(\"Train Dataset Shape: \", train_dataset.data.shape)\n",
    "print(\"Validation Dataset Shape: \", test_dataset.data.shape)\n",
    "\n",
    "# Ausgabe der Anzahl der Trainings- und Testbilder\n",
    "print(f\"Anzahl der Trainingsbilder: {len(train_dataset.data)}\")\n",
    "print(f\"Anzahl der Testbilder: {len(test_dataset.data)}\")\n",
    "\n",
    "# Dimension der Trainings- und Testbilder\n",
    "print(f\"Bildauflösung der Trainingsbilder: {train_dataset.data[0].shape}\")\n",
    "print(f\"Bildauflösung der Testbilder: {test_dataset.data[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualisierung der Daten \n",
    "\n",
    "# Ausgabe der Bilder des MNIST-Datasets\n",
    "def visualize_mnist_grid(dataset = train_dataset, title = 'MNIST Dataset'):\n",
    "    # Erstellen eines 10x10-Subplots\n",
    "    fig, axs = plt.subplots(10, 10, figsize=(10, 10))\n",
    "\n",
    "    # Durchlaufen der Subplots und Zuordnung der Bilder\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            # Finden des nächsten Bildes mit der Klasse i\n",
    "            idx = np.where(dataset.targets == i)[0][j]\n",
    "            # Laden des Bildes\n",
    "            img = dataset.data[idx]\n",
    "            # Anzeigen des Bildes\n",
    "            axs[i, j].imshow(img, cmap='gray')\n",
    "            axs[i, j].set_title(f\"{i} : {idx}\")\n",
    "            axs[i, j].axis('off')\n",
    "            fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # Einstellen des Layouts und Anzeigen des Grids\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Funktion ausführen\n",
    "visualize_mnist_grid(train_dataset, \"MNIST Train Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualisierung der Daten  \n",
    "\n",
    "# visualize distritbution of classes with bar chart for train and test dataset\n",
    "def visualize_mnist_bar(dataset_train = train_dataset, dataset_test = test_dataset, title = 'MNIST Dataset'):\n",
    "    # Erstellen eines 10x10-Subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Durchlaufen der Subplots und Zuordnung der Bilder\n",
    "    for i in range(2):\n",
    "        if i == 0:\n",
    "            axs[i].bar(np.unique(dataset_train.targets), np.unique(dataset_train.targets, return_counts=True)[1], color='blue', alpha=0.5, linewidth=1, edgecolor='black' )\n",
    "            axs[i].set_title(\"Train Dataset\")\n",
    "            # add numbers to bars\n",
    "            for j in range(len(np.unique(dataset_train.targets))):\n",
    "                axs[i].text(j, np.unique(dataset_train.targets, return_counts=True)[1][j], np.unique(dataset_train.targets, return_counts=True)[1][j], ha='center', va='bottom')\n",
    "        else:\n",
    "            axs[i].bar(np.unique(dataset_test.targets), np.unique(dataset_test.targets, return_counts=True)[1], color = \"blue\", alpha=0.5, linewidth=1, edgecolor='black' )\n",
    "            axs[i].set_title(\"Test Dataset\")\n",
    "            # add numbers to bars\n",
    "            for j in range(len(np.unique(dataset_test.targets))):\n",
    "                axs[i].text(j, np.unique(dataset_test.targets, return_counts=True)[1][j], np.unique(dataset_test.targets, return_counts=True)[1][j], ha='center', va='bottom')\n",
    "    \n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    # Einstellen des Layouts und Anzeigen des Grids\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Funktion ausführen\n",
    "visualize_mnist_bar(dataset_train = train_dataset, dataset_test = test_dataset, title = \"MNIST Dataset classes distribution\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Aufgabe 2\n",
    "Erstellen Sie eine Klasse für ein lineares Layer mit beliebig vielen Knoten. Implementieren Sie die\n",
    "Methoden forward, backward und update mithilfe von numpy. Schreiben sie geeignete Unittests,\n",
    "um die Funktionsweise der Funktion zu prüfen.\n",
    "\n",
    "**Linear Layer**\n",
    "\n",
    "4) Die Klasse für ein lineares Layer wurde mit beliebig vielen Knoten korrekt implementiert.\n",
    "5) Es wurden geeignete Unittests geschrieben, um die Funktionsweise der Klasse zu prüfen.\n",
    "(Richtige Berechnung des Gradienten bei mind. zwei Datenpunkten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Klasse Linear Layer mit beliebig vielen Knoten implementieren\n",
    "\n",
    "class LinearLayer:\n",
    "    \"\"\"\n",
    "    Eine Lineare Schicht in einem neuronalen Netzwerk. Die Schicht besteht aus einer Matrix von Gewichten,\n",
    "    einem Vektor von Biaswerten und Methoden zur Vorwärts- und Rückwärtsdurchlauf durch die Schicht.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, activation_fn=None, activation_fn_derivative=None):\n",
    "        \"\"\"\n",
    "        Initialisiert die Gewichte und Biaswerte der Schicht.\n",
    "        \n",
    "        :param input_dim: Dimension des Eingabevektors\n",
    "        :param output_dim: Dimension des Ausgabevektors\n",
    "        :param activation_fn: Aktivierungsfunktion\n",
    "        :param activation_fn_derivative: Ableitung der Aktivierungsfunktion\n",
    "        \"\"\"\n",
    "        \n",
    "        #xavier initialization\n",
    "        bound = np.sqrt(6) / np.sqrt(input_dim + output_dim) \n",
    "        self.weights = np.random.uniform(-bound, bound, (output_dim, input_dim))\n",
    "        self.bias = np.random.uniform(-bound, bound, (1, output_dim))\n",
    "        \n",
    "        self.input = None\n",
    "        self.activation_fn = activation_fn\n",
    "        self.activation_fn_derivative = activation_fn_derivative\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Führt einen Vorwärtsdurchlauf durch die Schicht aus.\n",
    "\n",
    "        :param input: Eingabevektor mit Dimension (batch_size, input_dim)\n",
    "        :return: Ausgabevektor mit Dimension (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        if self.activation_fn:\n",
    "            return self.activation_fn(np.dot(input, self.weights.T) + self.bias)\n",
    "        \n",
    "        return np.dot(input, self.weights.T) + self.bias\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Führt einen Rückwärtsdurchlauf durch die Schicht aus und berechnet die Gradienten von Eingabe, Gewichten und Bias.\n",
    "        \n",
    "        :param grad_output: Gradienten der Ausgabe mit Dimension (batch_size, output_dim)\n",
    "        :return: Gradienten der Eingabe mit Dimension (batch_size, input_dim), \n",
    "                 Gradienten der Gewichte mit Dimension (output_dim, input_dim),\n",
    "                 Gradienten des Bias mit Dimension (1, output_dim)\n",
    "        \"\"\"\n",
    "        if self.activation_fn_derivative:\n",
    "            grad_input = np.dot(grad_output, self.weights) * self.activation_fn_derivative(self.input)\n",
    "        else:\n",
    "            grad_input = np.dot(grad_output, self.weights)\n",
    "            \n",
    "        self.grad_weights = np.dot(grad_output.T, self.input)\n",
    "        self.grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "\n",
    "        return grad_input\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Aktualisiert die Gewichte und den Versatz der Schicht mit gegebener Lernrate und Gradienten.\n",
    "\n",
    "        :param learning_rate: Lernrate, die für die Aktualisierung verwendet wird.\n",
    "        \"\"\"\n",
    "        # Anpassung der Gewichte und Versätze mit gegebener Lernrate und Gradienten\n",
    "        self.weights -= learning_rate * self.grad_weights\n",
    "        self.bias -= learning_rate * self.grad_bias\n",
    "\n",
    "#(Generiert von ChatGPT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearLayer Klasse\n",
    "\n",
    "Die `LinearLayer` Klasse repräsentiert eine lineare Schicht in einem neuronalen Netzwerk. Diese Schicht besteht aus einer Matrix von Gewichten, einem Vektor von Biaswerten und Methoden zur Vorwärts- und Rückwärtsdurchlauf durch die Schicht.\n",
    "\n",
    "### Konstruktor\n",
    "\n",
    "Der Konstruktor der Klasse `LinearLayer` initialisiert die Gewichte und Biaswerte der Schicht. Dabei wird eine zufällige Initialisierung mit einer Standardabweichung von `1/sqrt(output_dim)` verwendet.\n",
    "\n",
    "- `input_dim`: Die Dimension des Eingabevektors.\n",
    "- `output_dim`: Die Dimension des Ausgabevektors.\n",
    "- `activation_fn`: Die Aktivierungsfunktion der Schicht.\n",
    "- `activation_fn_derivative`: Die Ableitung der Aktivierungsfunktion.\n",
    "\n",
    "### forward Methode\n",
    "\n",
    "Die `forward` Methode führt einen Vorwärtsdurchlauf durch die Schicht aus. Dabei wird der Eingabevektor mit den Gewichten multipliziert und der Biasvektor addiert. Falls eine Aktivierungsfunktion vorhanden ist, wird sie angewendet.\n",
    "\n",
    "- `input`: Der Eingabevektor mit der Dimension `(batch_size, input_dim)`.\n",
    "- Rückgabe: Der Ausgabevektor mit der Dimension `(batch_size, output_dim)`.\n",
    "\n",
    "### backward Methode\n",
    "\n",
    "Die `backward` Methode führt einen Rückwärtsdurchlauf durch die Schicht aus und berechnet die Gradienten von Eingabe, Gewichten und Bias. Dabei wird der Gradienten der Ausgabe mit der Transponierten der Gewichtsmatrix multipliziert und falls eine Aktivierungsfunktion vorhanden ist, wird ihre Ableitung mit dem resultierenden Vektor multipliziert.\n",
    "\n",
    "- `grad_output`: Der Gradienten der Ausgabe mit der Dimension `(batch_size, output_dim)`.\n",
    "- Rückgabe: Der Gradienten der Eingabe mit der Dimension `(batch_size, input_dim)`, der Gradienten der Gewichte mit der Dimension `(output_dim, input_dim)` und der Gradienten des Bias mit der Dimension `(1, output_dim)`.\n",
    "\n",
    "### update Methode\n",
    "\n",
    "Die `update` Methode aktualisiert die Gewichte und Bias der Schicht mit gegebener Lernrate und Gradienten.\n",
    "\n",
    "- `learning_rate`: Die Lernrate, die für die Aktualisierung verwendet wird.\n",
    "\n",
    "`Generiert von ChatGPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Geeigneter Unittest geschrieben und Funktionsweise der Klasse geprüft\n",
    "\n",
    "class TestLinearLayer(unittest.TestCase):\n",
    "    \n",
    "    def test_forward_output_shape(self):\n",
    "        # Testet, ob die Ausgabe der Vorwärtsdurchlaufmethode die erwartete Form hat\n",
    "        input_dim = 2\n",
    "        output_dim = 3\n",
    "        layer = LinearLayer(input_dim, output_dim)\n",
    "        x = np.random.randn(4, input_dim)\n",
    "        output = layer.forward(x)\n",
    "        expected_output_shape = (4, output_dim)\n",
    "        self.assertEqual(output.shape, expected_output_shape)\n",
    "    \n",
    "    def test_backward_output_shape(self):\n",
    "        # Testet, ob die Ausgabe der Rückwärtsdurchlaufmethode die erwartete Form hat\n",
    "        input_dim = 2\n",
    "        output_dim = 3\n",
    "        layer = LinearLayer(input_dim, output_dim)\n",
    "        x = np.random.randn(4, input_dim)\n",
    "        output = layer.forward(x)\n",
    "        grad_output = np.random.randn(4, output_dim)\n",
    "        grad_input = layer.backward(grad_output)\n",
    "        expected_grad_input_shape = (4, input_dim)\n",
    "        self.assertEqual(grad_input.shape, expected_grad_input_shape)\n",
    "\n",
    "# Lade die Testfunktionen\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestLinearLayer)\n",
    "\n",
    "# Führe die Tests aus\n",
    "unittest.TextTestRunner().run(suite)\n",
    "\n",
    "#(Generiert von ChatGPT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestLinearLayer Klasse\n",
    "\n",
    "Die `TestLinearLayer` Klasse ist eine Unterklasse von `unittest.TestCase` und enthält Tests für die `LinearLayer` Klasse.\n",
    "\n",
    "### test_forward_output_shape Methode\n",
    "\n",
    "Die `test_forward_output_shape` Methode testet, ob die Ausgabe der Vorwärtsdurchlaufmethode die erwartete Form hat. Dabei wird eine Instanz der `LinearLayer` Klasse mit gegebener Eingabe- und Ausgabedimension erstellt. Dann wird eine zufällige Eingabe erstellt und die `forward` Methode aufgerufen. Die Form der Ausgabe wird mit der erwarteten Form verglichen.\n",
    "\n",
    "### test_backward_output_shape Methode\n",
    "\n",
    "Die `test_backward_output_shape` Methode testet, ob die Ausgabe der Rückwärtsdurchlaufmethode die erwartete Form hat. Dabei wird eine Instanz der `LinearLayer` Klasse mit gegebener Eingabe- und Ausgabedimension erstellt. Dann wird eine zufällige Eingabe erstellt und die `forward` Methode aufgerufen. Ein zufälliger Gradienten der Ausgabe wird erstellt und die `backward` Methode aufgerufen. Die Form des Gradienten der Eingabe wird mit der erwarteten Form verglichen.\n",
    "\n",
    "### Test ausführen\n",
    "\n",
    "Die `suite` Variable wird verwendet, um die Testfunktionen zu laden. Dann wird die `TextTestRunner` Klasse verwendet, um die Tests auszuführen und die Ergebnisse in der Konsole auszugeben.\n",
    "\n",
    "`Generiert von ChatGPT`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Aufgabe 3\n",
    "Erstellen Sie ein neuronales Netzwerk in numpy mit einem Hidden Linear Layer und einem Output\n",
    "Knoten. Trainieren Sie das Netzwerk darauf, die Ziffer 4 korrekt zu identifizieren (d.h. der Output\n",
    "soll 1 für diese Ziffer und 0 für alle anderen Ziffern sein). Trainieren Sie das Netzwerk auf den\n",
    "Trainingsdaten und evaluieren Sie es anhand von Testdaten. Verwenden Sie eine geeignete Loss-\n",
    "Funktion sowie Accuracy-Funktion und geben Sie deren mathematische Definition an. Begründen\n",
    "Sie Ihre Wahl mit einer Abwägung der Vor- und Nachteile. Diskutieren Sie kurz weitere Optionen\n",
    "für Loss und Accuracy.\n",
    "\n",
    "**Single Layer Model**\n",
    "\n",
    "6) Das neuronale Netzwerk wurde mit einem Hidden Layer beliebiger Grösse und einem Output\n",
    "Knoten korrekt implementiert.\n",
    "7) Geeignete Loss- und Accuracy-Funktionen wurden verwendet.\n",
    "8) Die Wahl wurde begründet und mit anderen mögliche Funktionen verglichen?\n",
    "9) Die mathematische Definition der Loss-Funktion und Accuracy-Funktion ist korrekt angegeben\n",
    "(gerendert in Latex)?\n",
    "10) Die geeignete Loss-Funktion und Accuracy-Funktion wurde korrekt implementiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Neuronales Netzwerk mit einem Hidden Layer und beliebger Größe \n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid-Aktivierungsfunktion.\n",
    "    \"\"\"\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Wendet die Sigmoid-Aktivierungsfunktion auf eine gegebene Eingabe an.\n",
    "        \n",
    "        :param x: Eingabe\n",
    "        :return: Ausgabe der Sigmoid-Aktivierungsfunktion\n",
    "        \"\"\"\n",
    "        x_clipped = np.clip(x, -709, 709) # Begrenze die Werte von x, um Überlauf zu vermeiden\n",
    "        return 1 / (1 + np.exp(-x_clipped))\n",
    "\n",
    "    def derivative(self, x):\n",
    "        \"\"\"\n",
    "        Berechnet die Ableitung der Sigmoid-Aktivierungsfunktion an der gegebenen Stelle.\n",
    "        \n",
    "        :param x: Stelle, an der die Ableitung berechnet werden soll\n",
    "        :return: Ableitung der Sigmoid-Aktivierungsfunktion an der gegebenen Stelle\n",
    "        \"\"\"\n",
    "        sigmoid_x = self.__call__(x)\n",
    "        return sigmoid_x * (1 - sigmoid_x)\n",
    "    \n",
    "class WeightedBinaryCrossEntropyLoss:\n",
    "    \"\"\"\n",
    "    Eine Loss-Funktion, die die Weighted Binary Cross Entropy Loss auf die Ausgabe des neuronalen Netzwerks anwendet.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_pos, weight_neg):\n",
    "        \"\"\"\n",
    "        Initialisiert die Gewichte für die Weighted Binary Cross Entropy Loss.\n",
    "\n",
    "        :param weight_pos: Gewicht für die positive Klasse (Minderheitsklasse)\n",
    "        :param weight_neg: Gewicht für die negative Klasse (Mehrheitsklasse)\n",
    "        \"\"\"\n",
    "        self.weight_pos = weight_pos\n",
    "        self.weight_neg = weight_neg\n",
    "        \n",
    "    def __call__(self, output, target):\n",
    "        \"\"\"\n",
    "        Berechnet die Weighted Binary Cross Entropy Loss auf die Ausgabe des neuronalen Netzwerks.\n",
    "        \n",
    "        :param output: Ausgabe des neuronalen Netzwerks\n",
    "        :param target: Zielwerte\n",
    "        :return: Weighted Binary Cross Entropy Loss\n",
    "        \"\"\"\n",
    "        # clip values to avoid numerical instability\n",
    "        output = np.clip(output, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        # compute loss with weighted cross-entropy\n",
    "        loss = - (self.weight_pos * target * np.log(output) + self.weight_neg * (1 - target) * np.log(1 - output))\n",
    "        loss = np.mean(loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def gradient(self, output, target):\n",
    "        \"\"\"\n",
    "        Berechnet den Gradienten der Weighted Binary Cross Entropy Loss auf die Ausgabe des neuronalen Netzwerks.\n",
    "        \n",
    "        :param output: Ausgabe des neuronalen Netzwerks\n",
    "        :param target: Zielwerte\n",
    "        :return: Gradient der Weighted Binary Cross Entropy Loss auf die Ausgabe des neuronalen Netzwerks\n",
    "        \"\"\"\n",
    "        # clip values to avoid numerical instability\n",
    "        output = np.clip(output, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        # compute gradient with weighted cross-entropy\n",
    "        grad = - (self.weight_pos * target / output - self.weight_neg * (1 - target) / (1 - output))\n",
    "        grad = grad / target.size\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Ein einfaches neuronales Netzwerk mit einer linearen Schicht und einer tanh-Aktivierungsfunktion.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nodes, hidden_nodes, weight_pos, weight_neg):\n",
    "        \"\"\"\n",
    "        Initialisiert die lineare Schicht und die Aktivierungsfunktion des neuronalen Netzwerks.\n",
    "\n",
    "        :param input_nodes: Anzahl der Input-Nodes\n",
    "        :param hidden_nodes: Anzahl der Hidden-Nodes\n",
    "        :param weight_pos: Gewicht für die positive Klasse (Minderheitsklasse)\n",
    "        :param weight_neg: Gewicht für die negative Klasse (Mehrheitsklasse)\n",
    "        \"\"\"\n",
    "        self.layer1 = LinearLayer(input_nodes, hidden_nodes, activation_fn=Sigmoid(), activation_fn_derivative=Sigmoid().derivative)\n",
    "        self.layer2 = LinearLayer(hidden_nodes, 1, activation_fn=Sigmoid(), activation_fn_derivative=Sigmoid().derivative)\n",
    "        self.loss_fn = WeightedBinaryCrossEntropyLoss(weight_pos, weight_neg)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Führt einen Vorwärtsdurchlauf durch das neuronale Netzwerk durch.\n",
    "\n",
    "        :param input: Eingabevektor\n",
    "        :return: Ausgabevektor des neuronalen Netzwerks\n",
    "        \"\"\"\n",
    "        out = self.layer1.forward(input)\n",
    "        out = self.layer2.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "        Führt einen Rückwärtsdurchlauf durch das neuronale Netzwerk durch und berechnet die Gradienten\n",
    "        für die Gewichte und den Bias der linearen Schichten.\n",
    "\n",
    "        :param input: Eingabevektor\n",
    "        :param target: Zielwerte\n",
    "        :return: das aktualisierte neuronale Netzwerk\n",
    "        \"\"\"\n",
    "        # Vorwärtsdurchlauf\n",
    "        output = self.forward(input)\n",
    "\n",
    "        # Berechne Loss und Gradienten\n",
    "        grad_loss = self.loss_fn.gradient(output, target)\n",
    "\n",
    "        # Rückwärtsdurchlauf\n",
    "        grad_output = self.layer2.backward(grad_loss)\n",
    "        grad_input = self.layer1.backward(grad_output)\n",
    "\n",
    "        # Rückgabe des aktualisierten neuronalen Netzwerks\n",
    "        return self\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Aktualisiert die Gewichte und den Bias des neuronalen\n",
    "            :param learning_rate: Lernrate\n",
    "        \"\"\"\n",
    "        self.layer1.update(learning_rate)\n",
    "        self.layer2.update(learning_rate)\n",
    "\n",
    "    def loss(self, input, target):\n",
    "        \"\"\"\n",
    "        Berechnet den Loss des neuronalen Netzwerks auf einer gegebenen Eingabe und den zugehörigen Zielwerten.\n",
    "\n",
    "        :param input: Eingabevektor\n",
    "        :param target: Zielwerte\n",
    "        :return: Loss des neuronalen Netzwerks auf der gegebenen Eingabe und den zugehörigen Zielwerten\n",
    "        \"\"\"\n",
    "        output = self.forward(input)\n",
    "        return self.loss_fn(output, target)\n",
    "    \n",
    "    def accuracy(self, input, target, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Berechnet die Genauigkeit des neuronalen Netzwerks auf einer gegebenen Eingabe und den zugehörigen Zielwerten.\n",
    "\n",
    "        :param input: Eingabevektor\n",
    "        :param target: Zielwerte\n",
    "        :param threshold: Schwellenwert für die Klassifikation\n",
    "        :return: Genauigkeit des neuronalen Netzwerks auf der gegebenen Eingabe und den zugehörigen Zielwerten\n",
    "        \"\"\"\n",
    "        output = self.forward(input)\n",
    "        output[output >= threshold] = 1\n",
    "        output[output < threshold] = 0\n",
    "        hits = np.sum(output == target)\n",
    "        size = target.size\n",
    "        return hits / size\n",
    "    \n",
    "    def weighted_accuracy(self, input, target, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Berechnet die gewichtete Genauigkeit des neuronalen Netzwerks auf einer gegebenen Eingabe und den zugehörigen Zielwerten.\n",
    "\n",
    "        :param input: Eingabevektor\n",
    "        :param target: Zielwerte\n",
    "        :param threshold: Schwellenwert für die Klassifikation\n",
    "        :return: Gewichtete Genauigkeit des neuronalen Netzwerks auf der gegebenen Eingabe und den zugehörigen Zielwerten\n",
    "        \"\"\"\n",
    "        output = self.forward(input)\n",
    "        output[output >= threshold] = 1\n",
    "        output[output < threshold] = 0\n",
    "        tp = np.sum(output * target)\n",
    "        tn = np.sum((1 - output) * (1 - target))\n",
    "        fp = np.sum(output * (1 - target))\n",
    "        fn = np.sum((1 - output) * target)\n",
    "        accuracy_class1 = tp / (tp + fn)\n",
    "        accuracy_class0 = tn / (tn + fp)\n",
    "        return (accuracy_class1 + accuracy_class0) / 2\n",
    "\n",
    "    def f1_score(self, input, target, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Berechnet den F1-Score des neuronalen Netzwerks auf einer gegebenen Eingabe und den zugehörigen Zielwerten.\n",
    "\n",
    "        :param input: Eingabevektor\n",
    "        :param target: Zielwerte\n",
    "        :param threshold: Schwellenwert für die Klassifikation\n",
    "        :return: F1-Score des neuronalen Netzwerks auf der gegebenen Eingabe und den zugehörigen Zielwerten\n",
    "        \"\"\"\n",
    "        output = self.forward(input)\n",
    "        output[output >= threshold] = 1\n",
    "        output[output < threshold] = 0\n",
    "        tp = np.sum(output * target)\n",
    "        fp = np.sum(output * (1 - target))\n",
    "        fn = np.sum((1 - output) * target)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "#(Generiert von ChatGPT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klassenbeschreibungen\n",
    "\n",
    "### Klasse Sigmoid\n",
    "Eine Klasse, die eine Sigmoid-Aktivierungsfunktion repräsentiert. Sie hat eine `__call__`-Methode, die die Sigmoid-Aktivierungsfunktion auf eine gegebene Eingabe anwendet, und eine `derivative`-Methode, die die Ableitung der Sigmoid-Aktivierungsfunktion an einer gegebenen Stelle berechnet.\n",
    "\n",
    "### Klasse WeightedBinaryCrossEntropyLoss\n",
    "Eine Klasse, die eine gewichtete Binary Cross Entropy Loss-Funktion repräsentiert. Sie hat eine `__init__`-Methode, um die Gewichte für die positive und negative Klasse (Minderheitsklasse und Mehrheitsklasse) zu initialisieren, und eine `__call__`-Methode, um die gewichtete Binary Cross Entropy Loss-Funktion auf die Ausgabe des neuronalen Netzwerks anzuwenden. Sie hat auch eine `gradient`-Methode, um den Gradienten der gewichteten Binary Cross Entropy Loss-Funktion auf die Ausgabe des neuronalen Netzwerks zu berechnen.\n",
    "\n",
    "### Klasse NeuralNetwork\n",
    "Eine Klasse, die ein einfaches neuronales Netzwerk mit einer linearen Schicht und einer tanh-Aktivierungsfunktion repräsentiert. Sie hat eine `__init__`-Methode, um die lineare Schicht und die Aktivierungsfunktion des neuronalen Netzwerks zu initialisieren. Sie hat auch eine `forward`-Methode, um einen Vorwärtsdurchlauf durch das neuronale Netzwerk durchzuführen, und eine `backward`-Methode, um einen Rückwärtsdurchlauf durch das neuronale Netzwerk durchzuführen und die Gradienten für die Gewichte und den Bias der linearen Schichten zu berechnen. Sie hat auch eine `update`-Methode, um die Gewichte und den Bias des neuronalen Netzwerks zu aktualisieren. Außerdem hat sie Methoden zur Berechnung des Losses, der Genauigkeit, der gewichteten Genauigkeit und des F1-Scores des neuronalen Netzwerks auf einer gegebenen Eingabe und den zugehörigen Zielwerten.\n",
    "\n",
    "`Generiert von ChatGPT`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsere Wahl der Loss-funktion\n",
    "\n",
    "Die Binary Cross Entropy Loss (BCE) wird üblicherweise verwendet, um die Klassifizierungsergebnisse bei einer binären Klassifikation zu bewerten. Sie ist in der Lage, den Fehler zwischen den Vorhersagen des neuronalen Netzwerks und den tatsächlichen Labels zu quantifizieren. Die BCE Loss berechnet den logarithmischen Fehler zwischen der Vorhersage und dem Label. Bei einer binären Klassifikation ist dies ein Skalarwert, der den Fehler der Vorhersage des Netzwerks angibt.\n",
    "\n",
    "Die Weighted Binary Cross Entropy Loss (WBCE) ist eine Variation der BCE Loss, die darauf abzielt, Ungleichgewichte zwischen den Klassen auszugleichen, insbesondere bei einer ungleich verteilten Datenmenge. Die WBCE Loss weist den Klassen unterschiedliche Gewichte zu, um die Beiträge von Klassen mit unterschiedlicher Häufigkeit auszugleichen. In der Regel wird die Klasse, die seltener vorkommt, ein höheres Gewicht zugewiesen, während die häufigere Klasse ein geringeres Gewicht erhält. Dadurch wird der Einfluss der seltenen Klasse auf den Trainingsprozess verstärkt.\n",
    "\n",
    "Im Fall eines ungleich verteilten Datensatzes kann die WBCE Loss besser geeignet sein, da sie die Vorhersagen des Modells stärker auf die seltenen Klassen ausrichtet und somit die Leistung des Modells auf den seltenen Klassen verbessert. In unserem Fall kann die Verwendung von WBCE Loss helfen, die Leistung des Modells bei der Erkennung der seltenen Klasse zu verbessern und gleichzeitig die Leistung bei der häufigeren Klasse beizubehalten.\n",
    "\n",
    "`Generiert von ChatGPT`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Gewichte `weight_pos` und `weight_neg` sollten so gewählt werden, dass sie das Ungleichgewicht zwischen den Klassen ausgleichen und die Leistung des Modells auf der seltenen Klasse verbessern. Es gibt verschiedene Ansätze, um die Gewichte zu wählen:\n",
    "\n",
    "1. Manuelle Einstellung: In diesem Ansatz werden die Gewichte manuell durch den Benutzer festgelegt, basierend auf dem Verhältnis der Klassen in den Trainingsdaten. Die Gewichte können so gewählt werden, dass sie das Verhältnis der Klassen widerspiegeln, z.B. `weight_pos = 0.9` und `weight_neg = 0.1`, wenn die positive Klasse nur 10% der Trainingsdaten ausmacht.\n",
    "\n",
    "2. Automatische Einstellung: In diesem Ansatz werden die Gewichte automatisch durch das Modell während des Trainings angepasst. Dies kann durch Verwendung von Algorithmen wie der Weighted Random Sampler oder der Focal Loss erreicht werden, die die Gewichte während des Trainings anpassen, um das Modell stärker auf die seltenen Klassen auszurichten.\n",
    "\n",
    "Die Wahl der Gewichte hängt von der spezifischen Situation ab und erfordert möglicherweise einige Experimente, um die optimale Kombination von Gewichten zu finden, die zu einer verbesserten Leistung des Modells führen.\n",
    "\n",
    "`Generiert von ChatGPT`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematische Definitionen\n",
    "\n",
    "## Weighted binary cross entropy loss:\n",
    "Die Weighted Binary Cross Entropy Loss (WBCE) ist eine Erweiterung der Binary Cross Entropy Loss (BCE) Loss, die Klassen mit unterschiedlicher Häufigkeit in der Trainingsdatenmenge unterschiedliche Gewichte zuweist. \n",
    "\n",
    "Die mathematische Definition der WBCE Loss lautet:\n",
    "\n",
    "$$ WBCE = -\\frac{1}{N} \\sum_{i=1}^{N} [w_i \\cdot (y_i \\cdot log(p_i) + (1 - y_i) \\cdot log(1 - p_i))] $$\n",
    "\n",
    "wobei:\n",
    "- $N$ die Anzahl der Beispiele in der Trainingsdatenmenge ist\n",
    "- $w_i$ das Gewicht des $i$-ten Beispiels ist, definiert als $w_i = weight\\_pos$, wenn $y_i = 1$ (positive Klasse) und $w_i = weight\\_neg$, wenn $y_i = 0$ (negative Klasse)\n",
    "- $y_i$ das Label des $i$-ten Beispiels ist, entweder 0 oder 1, je nachdem ob das Beispiel zur negativen oder positiven Klasse gehört\n",
    "- $p_i$ die Vorhersage des Modells für das $i$-te Beispiel ist\n",
    "- $log()$ ist der natürliche Logarithmus\n",
    "\n",
    "Die WBCE Loss ist ähnlich zur BCE Loss, jedoch mit der zusätzlichen Berücksichtigung der Gewichte für die verschiedenen Klassen. Durch die Zuweisung von höheren Gewichten für die seltenere Klasse kann die WBCE Loss dazu beitragen, die Vorhersageleistung des Modells auf der seltenen Klasse zu verbessern.\n",
    "\n",
    "## Confusion matrix:\n",
    "Die Confusion Matrix (auch Verwirrungsmatrix genannt) ist eine Tabelle, die die Anzahl der Vorhersagen des Klassifikators für jede Klasse gegen die tatsächlichen Klassen enthält. Die Matrix hat normalerweise eine Größe von $C \\times C$, wobei $C$ die Anzahl der Klassen ist. Die folgende Gleichung zeigt eine allgemeine Darstellung der Confusion Matrix:\n",
    "\n",
    "|                    | Actually Positiv     | Actually Negative   |\n",
    "|--------------------|----------------------|---------------------|\n",
    "| Predicted Positiv  | True Positiv (TP)    | False Positiv (FP)  |\n",
    "| Predicted Negative | False Negatives (FN) | True Negatives (TN) |\n",
    "\n",
    "Dabei steht TP für die Anzahl der richtig vorhergesagten positiven Beispiele, FN für die Anzahl der falsch vorhergesagten negativen Beispiele, FP für die Anzahl der falsch vorhergesagten positiven Beispiele und TN für die Anzahl der richtig vorhergesagten negativen Beispiele. Die Confusion Matrix kann verwendet werden, um verschiedene Evaluationsmetriken zu berechnen, wie zum Beispiel Accuracy, Precision, Recall, F1-Score und weitere.\n",
    "\n",
    "## Accuracy:\n",
    "Die Accuracy (auch Korrektheitsrate genannt) gibt an, wie oft der Klassifikator korrekt vorhergesagt hat und wird als Prozentsatz der korrekten Vorhersagen berechnet.\n",
    "\n",
    "\\begin{equation}\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "\\end{equation}\n",
    "\n",
    "Wobei $TP$ die Anzahl der wahren positiven, $TN$ die Anzahl der wahren negativen, $FP$ die Anzahl der falsch positiven und $FN$ die Anzahl der falsch negativen Vorhersagen sind.\n",
    "\n",
    "## F1-Score:\n",
    "Der F1-Score ist das harmonische Mittel von Recall und Precision und gibt an, wie gut der Klassifikator sowohl bei der Identifizierung von relevanten als auch bei der Vermeidung von falsch positiven Ergebnissen ist.\n",
    "\n",
    "\\begin{equation}\n",
    "F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\n",
    "\\end{equation}\n",
    "\n",
    "Wobei der Recall (auch True Positive Rate genannt) angibt, wie viele relevante Elemente tatsächlich als solche erkannt wurden. Es ist das Verhältnis zwischen der Anzahl der tatsächlich positiven Elemente und der Summe aus tatsächlich positiven und falsch negativen Elementen.\n",
    "\n",
    "\\begin{equation}\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "\\end{equation}\n",
    "\n",
    "Wobei die Precision angibt, wie viele der als positiv klassifizierten Elemente tatsächlich relevant sind. Es ist das Verhältnis zwischen der Anzahl der tatsächlich positiven Elemente und der Summe aus tatsächlich positiven und falsch positiven Elementen.\n",
    "\n",
    "\\begin{equation}\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "\\end{equation}\n",
    "\n",
    "## Weighted Accuracy:\n",
    "Die gewichtete Genauigkeit (auch Balanced Accuracy genannt) ist ein Maß dafür, wie gut ein Klassifikator mit ungleicher Klassenverteilung funktioniert. Es ist das arithmetische Mittel der Genauigkeit der einzelnen Klassen.\n",
    "\n",
    "\\begin{equation}\n",
    "Weighted\\ Accuracy = \\frac{1}{2} \\cdot \\Bigg( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP} \\Bigg)\n",
    "\\end{equation}\n",
    "\n",
    "Wobei $TP$ die Anzahl der wahren positiven, $FN$ die Anzahl der falsch negativen, $TN$ die Anzahl der wahren negativen und $FP$ die Anzahl der falsch positiven Vorhersagen sind.\n",
    "\n",
    "`Generiert von ChatGPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Neuronales Netzwerk mit einem Hidden Layer und beliebger Größe \n",
    "\n",
    "# Erzeugen eines Beispiel Arrayys mit 3 Beobachtungen vom MNIST Datensatz\n",
    "input_array = np.array(train_dataset.data.view(-1, 28*28))\n",
    "\n",
    "# Umwandeln der Labels in numpy array\n",
    "# Transform Labels into 0 and 1 (everything else is 0 except 4, which is 1)\n",
    "target = np.where(np.array(train_dataset.targets.reshape(-1, 1)) == 4, 1, 0)\n",
    "\n",
    "# Array Ausgeben\n",
    "print(\"Input array:\\n\", input_array)\n",
    "print(\"Input array shape:\\n\", input_array.shape)\n",
    "\n",
    "# Array Labels\n",
    "print(\"Input array target:\\n\", target)\n",
    "print(\"Input array target shape:\\n\", target.shape)\n",
    "\n",
    "weight_pos = np.sum(target == 0) / len(target)\n",
    "weight_neg = np.sum(target == 1) / len(target)\n",
    "\n",
    "print(\"weight_pos: \", weight_pos)\n",
    "print(\"weight_neg: \", weight_neg)\n",
    "\n",
    "# Erzeugen einer Instanz \n",
    "net = NeuralNetwork(input_nodes=28*28, hidden_nodes=200, weight_pos=weight_pos, weight_neg=weight_neg)\n",
    "\n",
    "#beispiel für backprop\n",
    "loss = []\n",
    "for i in range(50):\n",
    "    loss.append(net.loss(input_array, target))\n",
    "    net.backward(input_array, target).update(1e-3)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print(f'Trainset | Epoch: {i} - Loss: {net.loss(input_array, target):.4f} - Accuracy: {net.accuracy(input_array, target):.4f} - Weighted Accuracy: {net.weighted_accuracy(input_array, target):.4f} - f1: {net.f1_score(input_array, target):.4f}')\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.plot(loss)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"train loss\")\n",
    "plt.title(\"Train loss vs epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Aufgabe 4\n",
    "Trainieren Sie das Netzwerk mit verschiedenen Lernraten und Größen des Hidden Layers. Verfolgen\n",
    "Sie während des Trainings die Entwicklung der Loss- und Accuracy-Funktionen auf Trainingsund\n",
    "Testdatensätzen und entscheiden Sie, welche Wahl von Lernrate und Hidden Layer-Größe die\n",
    "besten Ergebnisse in geringster Zeit liefert.\n",
    "\n",
    "**Single Layer Model: Training**\n",
    "\n",
    "11) Das Netzwerk wurde korrekt auf den Trainingsdaten trainiert.\n",
    "12) Das Netzwerk wurde korrekt auf den Testdaten evaluiert.\n",
    "13) Es wurden verschiedene Lernraten und Größen des Hidden Layers sinnvoll ausprobiert.\n",
    "14) Die Entwicklung der Loss- und Accuracy-Funktionen wurden auf Trainings- und Testdatensätzen\n",
    "korrekt verfolgt und leicht nachvollziehbar dargestellt?\n",
    "1\n",
    "15) Die Wahl von Lernrate und Hidden Layer-Größe wurde nachvollziehbar entschieden und\n",
    "begründet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugen von numpy Train und numpy test\n",
    "train_mnist_X = np.array(train_dataset.data.view(-1, 28*28))\n",
    "test_mnist_X = np.array(test_dataset.data.view(-1, 28*28))\n",
    "\n",
    "# Umwandeln der Labels in numpy array\n",
    "train_mnist_y = np.array(train_dataset.targets.reshape(-1, 1))\n",
    "test_mnist_y = np.array(test_dataset.targets.reshape(-1, 1))\n",
    "\n",
    "# Transform Labels into one-hot encoded format\n",
    "train_mnist_y = np.where(train_mnist_y == 4, 1, 0)\n",
    "test_mnist_y = np.where(test_mnist_y == 4, 1, 0)\n",
    "\n",
    "\n",
    "def train(model, epoch = 10, lr = 1e-6, plot_view=True, verbose=True, no_nodes=0):\n",
    "    epoch = epoch\n",
    "    loss_train_, loss_test_ = [], []\n",
    "    accuracy_train, accuracy_test = [], []\n",
    "    w_accuracy_train, w_accuracy_test = [], []\n",
    "    f1_train_, f1_test_ = [], []\n",
    "\n",
    "    for i in range(epoch):\n",
    "        loss_train = model.loss(train_mnist_X, train_mnist_y)\n",
    "        loss_test = model.loss(test_mnist_X, test_mnist_y)\n",
    "        loss_train_.append(loss_train)\n",
    "        loss_test_.append(loss_test)\n",
    "\n",
    "        train_acc = model.accuracy(train_mnist_X, train_mnist_y)\n",
    "        test_acc = model.accuracy(test_mnist_X, test_mnist_y)\n",
    "        accuracy_train.append(train_acc)\n",
    "        accuracy_test.append(test_acc)\n",
    "\n",
    "        train_w_acc = model.weighted_accuracy(train_mnist_X, train_mnist_y)\n",
    "        test_w_acc = model.weighted_accuracy(test_mnist_X, test_mnist_y)\n",
    "        w_accuracy_train.append(train_w_acc)\n",
    "        w_accuracy_test.append(test_w_acc)\n",
    "\n",
    "        f1_train = model.f1_score(train_mnist_X, train_mnist_y)\n",
    "        f1_test = model.f1_score(test_mnist_X, test_mnist_y)\n",
    "        f1_train_.append(f1_train)\n",
    "        f1_test_.append(f1_test)\n",
    "\n",
    "        # backprop\n",
    "        model.backward(train_mnist_X, train_mnist_y).update(learning_rate = lr)\n",
    "        if verbose:\n",
    "            if i % 5 == 0:\n",
    "                # 11. Netzwerk auf Trainigsdaten trainieren\n",
    "                # 12. Netzwerk auf Testdaten evaluieren\n",
    "                print(f\"Trainset | Epoch: {i+1}/{epoch} - loss: {loss_train:.4f} - accuracy: {train_acc:.4f} - w. accuracy: {train_w_acc:.4f} - f1: {f1_train:.4f}\")\n",
    "                print(f\"Testset | Epoch: {i+1}/{epoch} - loss: {loss_test:.4f} - accuracy: {test_acc:.4f} - w. accuracy: {test_w_acc:.4f} - f1: {f1_test:.4f}\")\n",
    "                print(\"-\" * 100)\n",
    "\n",
    "    if plot_view:\n",
    "        # plot loss and all metriks in 4 subplots\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axs[0, 0].plot(range(epoch), loss_train_, label='trainset', alpha = 0.5, color = \"red\")\n",
    "        axs[0, 0].plot(range(epoch), loss_test_, label='testset', alpha = 0.5, color = \"blue\")\n",
    "        axs[0, 0].set_title(\"Entwicklung der Loss Funktion mit {} Nodes im Hidden Layer und Learning Rate {}\".format(no_nodes, lr))\n",
    "        axs[0, 0].set_xlabel('Epoch')\n",
    "        axs[0, 0].set_ylabel('Loss')\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        axs[0, 1].plot(range(epoch), accuracy_train, label='trainset', alpha = 0.5, color = \"red\")\n",
    "        axs[0, 1].plot(range(epoch), accuracy_test, label='testset', alpha = 0.5, color = \"blue\")\n",
    "        axs[0, 1].set_title(\"Entwicklung der Accuracy mit {} Nodes im Hidden Layer und Learning Rate {}\".format(no_nodes, lr))\n",
    "        axs[0, 1].set_xlabel('Epoch')\n",
    "        axs[0, 1].set_ylabel('Accuracy')\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        axs[1, 0].plot(range(epoch), w_accuracy_train, label='trainset', alpha = 0.5, color = \"red\")\n",
    "        axs[1, 0].plot(range(epoch), w_accuracy_test, label='testset', alpha = 0.5, color = \"blue\")\n",
    "        axs[1, 0].set_title(\"Entwicklung der Weighted Accuracy mit {} Nodes im Hidden Layer und Learning Rate {}\".format(no_nodes, lr))\n",
    "        axs[1, 0].set_xlabel('Epoch')\n",
    "        axs[1, 0].set_ylabel('Weighted Accuracy')\n",
    "        axs[1, 0].legend()\n",
    "\n",
    "        axs[1, 1].plot(range(epoch), f1_train_, label='trainset', alpha = 0.5, color = \"red\")\n",
    "        axs[1, 1].plot(range(epoch), f1_test_, label='testset', alpha = 0.5, color = \"blue\")\n",
    "        axs[1, 1].set_title(\"Entwicklung der F1 Score mit {} Nodes im Hidden Layer und Learning Rate {}\".format(no_nodes, lr))\n",
    "        axs[1, 1].set_xlabel('Epoch')\n",
    "        axs[1, 1].set_ylabel('F1 Score')\n",
    "        axs[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Verschiedene Lernraten und Größen des Hidden Layers ausprobiert\n",
    "# 14. Entwicklung der Loss- und Accuracy-Funktionen wurden auf Trainings- und Testdatensätzen\n",
    "\n",
    "weight_pos = np.sum(train_mnist_y == 0) / len(train_mnist_y)\n",
    "weight_neg = np.sum(train_mnist_y == 1) / len(train_mnist_y)\n",
    "\n",
    "lst_learnrate = [2e-3, 4e-3, 8e-3]\n",
    "lst_hidden_nodes = [80, 150]\n",
    "no_epochs = 50\n",
    "\n",
    "for nomb_nodes in lst_hidden_nodes:\n",
    "    for lr_rate in lst_learnrate:\n",
    "        net = NeuralNetwork(input_nodes=28*28, hidden_nodes=nomb_nodes, weight_pos=weight_pos, weight_neg=weight_neg)\n",
    "        train(net, epoch = no_epochs, lr = lr_rate, plot_view = True, verbose=True, no_nodes=nomb_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Die Wahl von Lernrate und Hidden Layer-Größe wurde nachvollziehbar entschieden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Aufgabe 5\n",
    "Erweitern Sie das Netzwerk auf 3 Hidden Layer mit gleicher Größe und 10 Outputs. Das Ziel\n",
    "ist die korrekte Klassifizierung aller Ziffern. Verwenden Sie eine geeignete Loss-Funktion sowie\n",
    "Accuracy-Funktion und geben Sie deren mathematische Definition an. Begründen Sie Ihre Wahl\n",
    "und diskutieren Sie kurz weitere Möglichkeiten. Variieren Sie die Lernrate und die Größe der\n",
    "Hidden Layer und wählen Sie das beste Ergebnis aus.\n",
    "\n",
    "**Multi Layer Model**\n",
    "\n",
    "16) Das Netzwerk wurde auf 3 Hidden Layer mit gleicher, frei wählbarer Größe und 10 Outputs\n",
    "erweitert.\n",
    "17) Geeignete Loss- und Accuracy-Funktionen wurden verwendet.\n",
    "18) Die Wahl wurde begründet und mit anderen möglichen Funktionen verglichen.\n",
    "19) Die mathematische Definition der Loss-Funktion und Accuracy-Funktion ist korrekt angegeben\n",
    "(gerendert in Latex).\n",
    "20) Die geeigneten Loss- und Accuracy-Funktionen wurden korrekt implementiert.\n",
    "21) Es wurden verschiedene Lernraten und Größen der Hidden Layer sinnvoll ausprobiert.\n",
    "22) Die Entwicklung der Loss- und Accuracy-Funktionen auf Trainings- und Testdatensätzen\n",
    "wurde korrekt verfolgt und leicht nachvollziehbar dargestellt?\n",
    "23) Die Wahl der Hyperparameter wurde nachvollziehbar entschieden und begründet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Netzwerk auf 3 Hidden Layer mit gleicher, frei wählbarer Größe und 10 Outputs erweitern\n",
    "\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    The softmax function takes an array of scores as input and converts them to a probability distribution over the classes.\n",
    "    \"\"\"\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax of the input array.\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): The input array of shape (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: A probability distribution over the classes.\n",
    "        \"\"\"\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "# 17. Geeignete Loss- und Accuracy-Funktionen wurden verwendet.\n",
    "# 20. Korrekte Implementierung\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    \"\"\"\n",
    "    The cross-entropy loss measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "    \"\"\"\n",
    "    def __call__(self, output, target):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss.\n",
    "\n",
    "        Args:\n",
    "            output (numpy.ndarray): The output of the neural network, a probability distribution over the classes, of shape (batch_size, num_classes).\n",
    "            target (numpy.ndarray): The true labels, a one-hot encoded array of shape (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "            float: The average cross-entropy loss over the batch.\n",
    "        \"\"\"\n",
    "        output = np.clip(output, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean(np.sum(target * np.log(output), axis=1))\n",
    "\n",
    "    def gradient(self, output, target):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the cross-entropy loss with respect to the output.\n",
    "\n",
    "        Args:\n",
    "            output (numpy.ndarray): The output of the neural network, a probability distribution over the classes, of shape (batch_size, num_classes).\n",
    "            target (numpy.ndarray): The true labels, a one-hot encoded array of shape (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The gradient of the loss with respect to the output, of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        output = np.clip(output, 1e-15, 1 - 1e-15)\n",
    "        return output - target\n",
    "\n",
    "\n",
    "class NeuralNetworkV2:\n",
    "    \"\"\"\n",
    "    A neural network with multiple hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nodes, hidden_nodes):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with the given number of input nodes and hidden nodes.\n",
    "\n",
    "        Args:\n",
    "            input_nodes (int): The number of input nodes.\n",
    "            hidden_nodes (int): The number of hidden nodes per layer.\n",
    "        \"\"\"\n",
    "        self.layer1 = LinearLayer(input_nodes, hidden_nodes, activation_fn=Sigmoid(), activation_fn_derivative=Sigmoid().derivative)\n",
    "        self.layer2 = LinearLayer(hidden_nodes, hidden_nodes, activation_fn=Sigmoid(), activation_fn_derivative=Sigmoid().derivative)\n",
    "        self.layer3 = LinearLayer(hidden_nodes, hidden_nodes, activation_fn=Sigmoid(), activation_fn_derivative=Sigmoid().derivative)\n",
    "        self.layer4 = LinearLayer(hidden_nodes, 10, activation_fn=Softmax(), activation_fn_derivative=None)\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the neural network.\n",
    "\n",
    "        Args:\n",
    "            input (numpy.ndarray): The input to the neural network, of shape (batch_size, num_features).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The output of the neural network, a probability distribution over the classes, of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        out = self.layer1.forward(input)\n",
    "        out = self.layer2.forward(out)\n",
    "        out = self.layer3.forward(out)\n",
    "        out = self.layer4.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "        Performs a backward pass through the neural\n",
    "        \n",
    "        Args:\n",
    "            input (numpy.ndarray): The input to the neural network, of shape (batch_size, num_features).\n",
    "            target (numpy.ndarray): The true labels, a one-hot encoded array of shape (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "            NeuralNetworkV2: The neural network itself.\n",
    "        \"\"\"\n",
    "        output = self.forward(input)\n",
    "        grad_loss = self.loss_fn.gradient(output, target)\n",
    "        grad_output = self.layer4.backward(grad_loss)\n",
    "        grad_output = self.layer3.backward(grad_output)\n",
    "        grad_output = self.layer2.backward(grad_output)\n",
    "        grad_input = self.layer1.backward(grad_output)\n",
    "        return self\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the parameters of the neural network using gradient descent with the given learning rate.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): The learning rate.\n",
    "        \"\"\"\n",
    "        self.layer1.update(learning_rate)\n",
    "        self.layer2.update(learning_rate)\n",
    "        self.layer3.update(learning_rate)\n",
    "        self.layer4.update(learning_rate)\n",
    "\n",
    "    def loss(self, input, target):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss of the neural network on the given input and target.\n",
    "\n",
    "        Args:\n",
    "            input (numpy.ndarray): The input to the neural network, of shape (batch_size, num_features).\n",
    "            target (numpy.ndarray): The true labels, a one-hot encoded array of shape (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "            float: The average cross-entropy loss over the batch.\n",
    "        \"\"\"\n",
    "        output = self.forward(input)\n",
    "        return self.loss_fn(output, target)\n",
    "\n",
    "    def accuracy(self, input, target):\n",
    "        \"\"\"\n",
    "        Computes the accuracy of the neural network on the given input and target.\n",
    "\n",
    "        Args:\n",
    "            input (numpy.ndarray): The input to the neural network, of shape (batch_size, num_features).\n",
    "            target (numpy.ndarray): The true labels, a one-hot encoded array of shape (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "            float: The accuracy of the neural network on the given input and target.\n",
    "        \"\"\"\n",
    "        output = self.forward(input)\n",
    "        predictions = np.argmax(output, axis=1)\n",
    "        target_labels = np.argmax(target, axis=1)\n",
    "        hits = np.sum(predictions == target_labels)\n",
    "        size = target_labels.size\n",
    "        return hits / size\n",
    "\n",
    "    def f1_score(self, input, target):\n",
    "        \"\"\"\n",
    "        Computes the F1 score of the neural network on the given input and target.\n",
    "\n",
    "        Args:\n",
    "            input (numpy.ndarray): The input to the neural network, of shape (batch_size, num_features).\n",
    "            target (numpy.ndarray): The true labels, a one-hot encoded array of shape (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "            float: The weighted F1 score of the neural network on the given input and target.\n",
    "        \"\"\"\n",
    "        output = self.forward(input)\n",
    "        y_pred = np.argmax(output, axis=1)\n",
    "        y_true = np.argmax(target, axis=1)\n",
    "\n",
    "        tp = np.zeros_like(np.unique(y_true))\n",
    "        fp = np.zeros_like(np.unique(y_true))\n",
    "        fn = np.zeros_like(np.unique(y_true))\n",
    "\n",
    "        for i in range(len(np.unique(y_true))):\n",
    "            tp[i] = np.sum((y_pred == i) & (y_true == i))\n",
    "            fp[i] = np.sum((y_pred == i) & (y_true != i))\n",
    "            fn[i] = np.sum((y_pred != i) & (y_true == i))\n",
    "\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1_per_class = 2 * precision * recall / (precision + recall)\n",
    "        f1_per_class[np.isnan(f1_per_class)] = 0\n",
    "        weighted_f1 = np.mean(f1_per_class)\n",
    "\n",
    "        return weighted_f1\n",
    "\n",
    "    def weighted_accuracy(self, input, target):\n",
    "        \"\"\"\n",
    "        Computes the weighted accuracy of the neural network on the given input and target.\n",
    "\n",
    "        Args:\n",
    "            input (numpy.ndarray): The input to the neural network, of shape (batch_size, num_features).\n",
    "            target (numpy.ndarray): The true labels, a one-hot encoded array of shape (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "            float: The weighted accuracy of the neural network on the given input and target.\n",
    "        \"\"\"\n",
    "        output = self.forward(input)\n",
    "        y_pred = np.argmax(output, axis=1)\n",
    "        y_true = np.argmax(target, axis=1)\n",
    "\n",
    "        cm = np.zeros((len(np.unique(y_true)), len(np.unique(y_true))))\n",
    "        \n",
    "        for i, j in zip(y_true, y_pred):\n",
    "            cm[i, j] += 1\n",
    "\n",
    "        # Normalisiere die Zeilen der Konfusionsmatrix\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        # Berechne die gewichtete Genauigkeit\n",
    "        class_weights = np.sum(target, axis=0) / target.shape[0]\n",
    "        return np.sum(cm.diagonal() * class_weights)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klassenbeschreibung\n",
    "\n",
    "### Softmax\n",
    "\n",
    "Die `Softmax` Klasse implementiert die Softmax-Funktion, die eine Wahrscheinlichkeitsverteilung über den Klassen auf Basis eines Arrays von Scores berechnet.\n",
    "\n",
    "#### Methoden\n",
    "\n",
    "- `__call__(self, x)`: Berechnet die Softmax-Funktion auf Basis des Eingabe-Arrays `x` und gibt die Wahrscheinlichkeitsverteilung über den Klassen zurück.\n",
    "\n",
    "### CrossEntropyLoss\n",
    "\n",
    "Die `CrossEntropyLoss` Klasse implementiert die Cross-Entropy Loss Funktion, die die Performance eines Klassifikationsmodells misst, dessen Ausgabe eine Wahrscheinlichkeitsverteilung zwischen 0 und 1 ist.\n",
    "\n",
    "#### Methoden\n",
    "\n",
    "- `__call__(self, output, target)`: Berechnet die Cross-Entropy Loss auf Basis der Ausgabe des neuronalen Netzwerks `output` und der wahren Labels `target`.\n",
    "- `gradient(self, output, target)`: Berechnet den Gradienten der Cross-Entropy Loss Funktion in Bezug auf die Ausgabe des neuronalen Netzwerks `output`.\n",
    "\n",
    "### NeuralNetworkV2\n",
    "\n",
    "Die `NeuralNetworkV2` Klasse implementiert ein neuronales Netzwerk mit mehreren versteckten Schichten.\n",
    "\n",
    "#### Methoden\n",
    "\n",
    "- `__init__(self, input_nodes, hidden_nodes)`: Initialisiert das neuronale Netzwerk mit der gegebenen Anzahl von Eingabe- und versteckten Knoten.\n",
    "- `forward(self, input)`: Führt einen Forward-Pass durch das neuronale Netzwerk aus und gibt die Ausgabe als Wahrscheinlichkeitsverteilung über den Klassen zurück.\n",
    "- `backward(self, input, target)`: Führt einen Backward-Pass durch das neuronale Netzwerk aus, berechnet den Gradienten der Loss-Funktion und gibt das aktualisierte neuronale Netzwerk zurück.\n",
    "- `update(self, learning_rate)`: Aktualisiert die Parameter des neuronalen Netzwerks mithilfe von Gradientenabstieg und der gegebenen Lernrate.\n",
    "- `loss(self, input, target)`: Berechnet die Cross-Entropy Loss des neuronalen Netzwerks auf der gegebenen Eingabe und den wahren Labels.\n",
    "- `accuracy(self, input, target)`: Berechnet die Genauigkeit des neuronalen Netzwerks auf der gegebenen Eingabe und den wahren Labels.\n",
    "- `f1_score(self, input, target)`: Berechnet den gewichteten F1-Score des neuronalen Netzwerks auf der gegebenen Eingabe und den wahren Labels.\n",
    "- `weighted_accuracy(self, input, target)`: Berechnet die gewichtete Genauigkeit des neuronalen Netzwerks auf der gegebenen Eingabe und den wahren Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Testen des Netzwerkes mit unterschiedlichen Anzahl Neuronen\n",
    "\n",
    "# Erzeugen von numpy Train und numpy test\n",
    "train_mnist_X = np.array(train_dataset.data.view(-1, 28*28))\n",
    "test_mnist_X = np.array(test_dataset.data.view(-1, 28*28))\n",
    "\n",
    "# Umwandeln der Labels in numpy array\n",
    "train_mnist_y = np.array(train_dataset.targets.reshape(-1, 1))\n",
    "test_mnist_y = np.array(test_dataset.targets.reshape(-1, 1))\n",
    "\n",
    "# Transform Labels into one-hot encoded format\n",
    "train_mnist_y = np.eye(10)[train_mnist_y].reshape(-1, 10)\n",
    "test_mnist_y = np.eye(10)[test_mnist_y].reshape(-1, 10)\n",
    "\n",
    "def trainV2(model, epoch = 50, lr = 1e-6, plot_view = True, verbose = True, no_nodes=0):\n",
    "    epoch = epoch\n",
    "    loss_train_, loss_test_ = [], []\n",
    "    accuracy_train, accuracy_test = [], []\n",
    "    w_accuracy_train, w_accuracy_test = [], []\n",
    "    f1_train_, f1_test_ = [], []\n",
    "\n",
    "    for i in range(epoch):\n",
    "        loss_train = model.loss(train_mnist_X, train_mnist_y)\n",
    "        loss_test = model.loss(test_mnist_X, test_mnist_y)\n",
    "        loss_train_.append(loss_train)\n",
    "        loss_test_.append(loss_test)\n",
    "\n",
    "        train_acc = model.accuracy(train_mnist_X, train_mnist_y)\n",
    "        test_acc = model.accuracy(test_mnist_X, test_mnist_y)\n",
    "        accuracy_train.append(train_acc)\n",
    "        accuracy_test.append(test_acc)\n",
    "\n",
    "        train_w_acc = model.weighted_accuracy(train_mnist_X, train_mnist_y)\n",
    "        test_w_acc = model.weighted_accuracy(test_mnist_X, test_mnist_y)\n",
    "        w_accuracy_train.append(train_w_acc)\n",
    "        w_accuracy_test.append(test_w_acc)\n",
    "\n",
    "        f1_train = model.f1_score(train_mnist_X, train_mnist_y)\n",
    "        f1_test = model.f1_score(test_mnist_X, test_mnist_y)\n",
    "        f1_train_.append(f1_train)\n",
    "        f1_test_.append(f1_test)\n",
    "\n",
    "        # backprop\n",
    "        model.backward(train_mnist_X, train_mnist_y).update(learning_rate = lr)\n",
    "        if verbose:\n",
    "            if i % 2 == 0:\n",
    "                print(f\"Trainset | Epoch: {i+1}/{epoch} - loss: {loss_train:.4f} - accuracy: {train_acc:.4f} - w. accuracy: {train_w_acc:.4f} - f1: {f1_train:.4f}\")\n",
    "                print(f\"Testset  | Epoch: {i+1}/{epoch} - loss: {loss_test:.4f} - accuracy: {test_acc:.4f} - w. accuracy: {test_w_acc:.4f} - f1: {f1_test:.4f}\")\n",
    "                print(\"-\" * 100)\n",
    "\n",
    "    # 22. Die Entwicklung der Loss- und Accuracy-Funktionen auf Trainings- und Testdatensätzen verfolgen\n",
    "\n",
    "    if plot_view:\n",
    "        # plot loss and all metriks in 4 subplots\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axs[0, 0].plot(range(epoch), loss_train_, label='trainset', alpha = 0.5, color = \"red\")\n",
    "        axs[0, 0].plot(range(epoch), loss_test_, label='testset', alpha = 0.5, color = \"blue\")\n",
    "        axs[0, 0].set_title(\"Entwicklung der Loss Funktion mit {} Nodes im Hidden Layer und Learning Rate {}\".format(no_nodes, lr))\n",
    "        axs[0, 0].set_xlabel('Epoch')\n",
    "        axs[0, 0].set_ylabel('Loss')\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        axs[0, 1].plot(range(epoch), accuracy_train, label='trainset', alpha = 0.5, color = \"red\")\n",
    "        axs[0, 1].plot(range(epoch), accuracy_test, label='testset', alpha = 0.5, color = \"blue\")\n",
    "        axs[0, 1].set_title(\"Entwicklung der Accuracy mit {} Nodes im Hidden Layer und Learning Rate {}\".format(no_nodes, lr))\n",
    "        axs[0, 1].set_xlabel('Epoch')\n",
    "        axs[0, 1].set_ylabel('Accuracy')\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        axs[1, 0].plot(range(epoch), w_accuracy_train, label='trainset', alpha = 0.5, color = \"red\")\n",
    "        axs[1, 0].plot(range(epoch), w_accuracy_test, label='testset', alpha = 0.5, color = \"blue\")\n",
    "        axs[1, 0].set_title(\"Entwicklung der Weighted Accuracy mit {} Nodes im Hidden Layer und Learning Rate {}\".format(no_nodes, lr))\n",
    "        axs[1, 0].set_xlabel('Epoch')\n",
    "        axs[1, 0].set_ylabel('Weighted Accuracy')\n",
    "        axs[1, 0].legend()\n",
    "\n",
    "        axs[1, 1].plot(range(epoch), f1_train_, label='trainset', alpha = 0.5, color = \"red\")\n",
    "        axs[1, 1].plot(range(epoch), f1_test_, label='testset', alpha = 0.5, color = \"blue\")\n",
    "        axs[1, 1].set_title(\"Entwicklung der F1 Score mit {} Nodes im Hidden Layer und Learning Rate {}\".format(no_nodes, lr))\n",
    "        axs[1, 1].set_xlabel('Epoch')\n",
    "        axs[1, 1].set_ylabel('F1 Score')\n",
    "        axs[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. Die Entwicklung der Loss- und Accuracy-Funktionen auf Trainings- und Testdatensätzen verfolgen\n",
    "\n",
    "lst_learnrate = [1e-6, 3e-6, 5e-6]\n",
    "lst_nodes = [50, 100, 250]\n",
    "no_epoch = 50\n",
    "\n",
    "for nomb_nodes in lst_nodes:\n",
    "    for learn_rate in lst_learnrate:\n",
    "        net = NeuralNetworkV2(input_nodes=28*28, hidden_nodes=nomb_nodes)\n",
    "        trainV2(net, epoch = no_epoch, lr = learn_rate, plot_view = True, verbose=True, no_nodes=nomb_nodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematische Definitionen\n",
    "\n",
    "19. Mathematische Definition der Loss Funktion und Acuracy-Funktion\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correctly classified samples}}{\\text{Total number of samples}}\n",
    "$$\n",
    " \n",
    "Weighted Accuracy:\n",
    "\n",
    "$$\n",
    "\\text{Weighted Accuracy} = \\sum_{i=1}^{C}w_i \\times \\frac{\\text{Number of correctly classified samples for class } i}{\\text{Total number of samples for class } i}\n",
    "$$\n",
    " \n",
    "wobei $C$ die Anzahl der Klassen ist und $w_i$ das Gewicht für die Klasse $i$ darstellt.\n",
    "\n",
    "$$\n",
    "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "​\n",
    "wobei\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    " \n",
    "und\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "​\n",
    "Cross Entropy Loss:\n",
    "\n",
    "$$\n",
    "\\text{Cross Entropy Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "wobei $N$ die Anzahl der Samples, $C$ die Anzahl der Klassen, $y_{ij}$ das Element in der $i$-ten Zeile und $j$-ten Spalte des Target-Arrays und $\\hat{y}_{ij}$ das Element in der $i$-ten Zeile und $j$-ten Spalte des Output-Arrays ist."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ChatGPT & Form\n",
    "Für diese Aufgaben kann ChatGPT als Ressource genutzt werden, um Fragen zu stellen oder bei\n",
    "Problemen Unterstützung zu erhalten. Vorraussetzung ist, dass Sie transparent kommunizieren,\n",
    "wo und wie Sie ChatGPT eingesetzt haben und welche Verbesserungen nötig waren. Sie können\n",
    "ChatGPT wie folgt nutzen:\n",
    "\n",
    "• Stellen Sie Ihre Fragen klar und präzise.  \n",
    "• Formulieren Sie Ihre Fragen so, dass sie spezifisch auf Ihre Probleme abgestimmt sind.  \n",
    "• Seien Sie geduldig und geben Sie ChatGPT genügend Zeit, um eine sinnvolle Antwort zu\n",
    "generieren.  \n",
    "• Verwenden Sie die Antworten von ChatGPT als eine Art von Hilfestellung und überprüfen\n",
    "Sie diese stets auf ihre Richtigkeit.\n",
    "\n",
    "Wenn Sie eine Frage stellen, die auf eine bestimmte Stelle in Ihrem Code Bezug nimmt, können\n",
    "Sie den Code zusammen mit Ihrer Frage bereitstellen.\n",
    "\n",
    "**Form**\n",
    "\n",
    "24) Das Notebook lässt sich komplett und fehlerfrei ausführen.\n",
    "25) Es werden nur die Angegebenen Pakete verwendet: numpy, matplotlib; torchvision nur für\n",
    "das Dataset.\n",
    "26) Die Ergebnisse werden gut verständlich kommuniziert und kritisch evaluiert.\n",
    "27) Die Grafiken sind vollständig beschriftet und ohne weiter Erläuterung verständlich.\n",
    "28) Der Code ist gut strukturiert und verständlich kommentiert.\n",
    "29) Die Ergebnisse werden am Ende des Notebooks so zusammengefasst, dass diese Zusammenfassung\n",
    "eigenständig verständlich ist.\n",
    "30) Das Lerntagebuch ist kurz und verständlich geschrieben, zeigt den Lernfortschritt auf und\n",
    "macht mit den Kommentaren deutlich, wie und wofür ChatGPT und andere Tools verwendet\n",
    "wurden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lerntagebuch\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b519adb5a3361c34536ceca665d8c09f200cb8374a4986c7ce79fcb6d8f71188"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
